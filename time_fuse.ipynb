{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-terrier\n",
    "%pip install repro-eval\n",
    "%pip install ranx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timo/code/CLEF2024-LongEval-CIR/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n",
      "/var/folders/tb/5vs85b2520s950r538g7zznr0000gn/T/ipykernel_18784/4119560510.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bm25_t4_strip['queryid']= bm25_t4_strip['queryid_t5']\n",
      "/var/folders/tb/5vs85b2520s950r538g7zznr0000gn/T/ipykernel_18784/4119560510.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bm25_t4_strip['docid']= bm25_t4_strip['docid_t5']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "\n",
    "bm25_t3 = pt.io.read_results('runs/CIR_BM25.t3')\n",
    "bm25_t4 = pt.io.read_results('runs/CIR_BM25.t4')\n",
    "bm25_t5 = pt.io.read_results('runs/CIR_BM25.t5')\n",
    "bm25_t4_extended = pd.read_csv('runs/CIR_BM25_extended.t4', sep=' ')\n",
    "\n",
    "bm25_t4_strip = bm25_t4_extended[bm25_t4_extended['queryid_t5'].notna() & bm25_t4_extended['docid_t5'].notna()]\n",
    "bm25_t4_strip['queryid']= bm25_t4_strip['queryid_t5']\n",
    "bm25_t4_strip['docid']= bm25_t4_strip['docid_t5']\n",
    "bm25_t4_strip = bm25_t4_strip[['queryid', 'docid', 'relevance', 'score', 'run']]\n",
    "bm25_t4_strip = bm25_t4_strip.rename(columns={'queryid': 'qid', 'docid': 'docno', 'relevance': 'rank', 'run': 'name'})\n",
    "\n",
    "qids = set(bm25_t5['qid']).intersection(set(bm25_t4_strip['qid']))\n",
    "bm25_t5 = bm25_t5[bm25_t5['qid'].isin(qids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216218/216218 [00:02<00:00, 103813.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Kendall's tau:  0.9049362485729147\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from repro_eval.Evaluator import RpdEvaluator\n",
    "\n",
    "\n",
    "def time_fuse(run_recent, run_old, _lambda=0.5):\n",
    "    qid_ranking_groups = run_old.groupby('qid')\n",
    "    qid_ranking_dict = {qid: list(ranking['docno']) for qid, ranking in qid_ranking_groups}\n",
    "    \n",
    "    def weigh(row):\n",
    "        if row['docno'] in qid_ranking_dict.get(row['qid']):\n",
    "            return row['score'] * _lambda ** 2\n",
    "        else:\n",
    "            return row['score'] * (1-_lambda) ** 2              \n",
    "    reranking = run_recent.copy()\n",
    "    \n",
    "    reranking['score'] = reranking.groupby('qid')['score'].transform(lambda x : x / x.max())\n",
    "    reranking['score'] = reranking.progress_apply(weigh, axis=1)\n",
    "    reranking = reranking.sort_values(['qid','score'], ascending=False).groupby('qid').head(10000)\n",
    "    reranking['rank'] = reranking.groupby('qid')['score'].rank(ascending=False).astype(int)\n",
    "    return reranking\n",
    "\n",
    "# _lambda = 0.5 + 1e-3\n",
    "_lambda = 0.5 + 1e-12\n",
    "bm25_t5_reranked_by_t4 = time_fuse(bm25_t5, bm25_t4_strip, _lambda=_lambda)\n",
    "pt.io.write_results(bm25_t5_reranked_by_t4, 'bm25_t5_reranked_by_t4', format='trec',run_name='bm25_t5_reranked_by_t4')\n",
    "pt.io.write_results(bm25_t5, 'bm25_t5', format='trec',run_name='bm25_t5')\n",
    "rpd_eval = RpdEvaluator(run_b_orig_path='bm25_t5', run_b_rep_path='bm25_t5_reranked_by_t4')\n",
    "correlations = rpd_eval.ktau_union().get('baseline')\n",
    "correlation_scores = [x for x in list(correlations.values()) if ~np.isnan(x)]\n",
    "print(\"Avg. Kendall's tau: \", sum(correlation_scores) / len(correlation_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Kendall's tau:  0.004377672896374601\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from repro_eval.Evaluator import RpdEvaluator\n",
    "from ranx import Run, fuse\n",
    "\n",
    "def filter_and_fuse(run_recent, old_runs: list):\n",
    "    qid_ranking_groups = run_recent.groupby('qid')\n",
    "    qid_ranking_dict_recent = {qid: pd.Series(ranking['score'].values, ranking['docno']).to_dict() for qid, ranking in qid_ranking_groups}\n",
    "    runs = [Run.from_dict(qid_ranking_dict_recent)]\n",
    "    for run_old in old_runs:\n",
    "        qid_ranking_groups = run_old.groupby('qid')\n",
    "        qid_ranking_dict_old = {qid: pd.Series(ranking['score'].values, ranking['docno']).to_dict() for qid, ranking in qid_ranking_groups}\n",
    "        for qid, ranking in qid_ranking_dict_old.items():\n",
    "            docs_recent = qid_ranking_dict_recent.get(qid).keys()\n",
    "            qid_ranking_dict_old[qid] = {docid: score for docid, score in ranking.items() if docid in docs_recent}\n",
    "        runs.append(Run.from_dict(qid_ranking_dict_old))\n",
    "    combined_run = fuse(runs = runs, method = \"rrf\")\n",
    "\n",
    "    return combined_run\n",
    "\n",
    "run = filter_and_fuse(bm25_t5, [bm25_t4_strip])\n",
    "run.save('bm25_t5_rrf_t4', kind='trec')\n",
    "rpd_eval = RpdEvaluator(run_b_orig_path='bm25_t5', run_b_rep_path='bm25_t5_rrf_t4')\n",
    "correlations = rpd_eval.ktau_union().get('baseline')\n",
    "correlation_scores = [x for x in list(correlations.values()) if ~np.isnan(x)]\n",
    "print(\"Avg. Kendall's tau: \", sum(correlation_scores) / len(correlation_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
