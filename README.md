# CLEF2024-LongEval-CIR

| Data split | identifier |
| --- | --- |
| 2023 training set | t_0 / WT |
| 2023 test set | t_1 / ST (2022_07) |
| 2023 test set | t_2 / LT (2022_09) |
| 2024 training set | t_3 / 2023_01 |
| 2024 test set | t_4 / 2023_06 |
| 2024 test set | t_5 / 2023_08 |

**Indices:**
- [ ] 1x small index based on relevant documents t_0-t_3 and Jüri's SQLite database
- [x] 1x large index based on t_3
- [x] 1x large index based on t_4
- [x] 1x large index based on t_5

**Submission:** Rankings based on t_4 and t_5 

_Baselines:_
BM25(t_3)
BM25(t_4)
BM25(t_5)

_Naive filters:_
- BM25 of t_4 + Remove "non-relevant" documents from t_0 to t_3
- BM25 of t_5 + Remove "non-relevant" documents from t_0 to t_3

_(Pseudo) relevance feedback:_

Three runs based on PRF

- [x] (Hybrid RF approach) six (intermediate) topics files to produce three runs based on RF and PRF
Two topic sets for t_3: one with topics overlaps in t_0-t_2 and t_3, the other one for new topics in t_3
Two topic sets for t_4: one with topics overlaps in t_0-t_3 and t_4, the other one for new topics in t_4
Two topic sets for t_5: one with topics overlaps in t_0-t_3 and t_5, the other one for new topics in t_5

Afterwards, merge run files 



## Experiments on t3:
### BM25 baselie
| P_10 | bpref | ndcg |
|---|---|---|
| 0.1623745819397993 | 0.43728103787875366 | 0.36375525364044387 |

### Naive filters:
| P_10 | bpref | ndcg |
|---|---|---|
| 0.15702341137123746 | 0.4389914588436536 | 0.35276277467835815 |

### BM25 + time fuse
- lost one topic because no document overlap between t3 and t2 in this ranking

| $\lambda$ | Avg. Kendall's tau to BM25 | P_10 | bpref | ndcg |
|---|---|---|---|---|
| 0.501 | 0.07894661726059203 | 0.16521739130434782 | 0.4369563935417115 | 0.36664537840465017 |
| 0.5001 | 0.4268032114101738 | 0.16270903010033447 | 0.4371686732635674 | 0.3640573561718566 |
| 0.50001 | 0.8274175099065462 | 0.16237458193979934 | 0.43726071550788775 | 0.36376264697682054 |
| 0.500001 | 0.9377921482960695 | 0.16237458193979934 | 0.43728103787875366 | 0.363756262948394 |
| 0.5000001 | 0.9530700020867431 | 0.16237458193979934 | 0.43728103787875366 | 0.36375643096784677 |
| 0.50000001 | 0.9545358759874004 | 0.16237458193979934 | 0.43728103787875366 | 0.3637561380776753 |
| 0.500000001 | 0.9547528153347746 | 0.16237458193979934 | 0.43728103787875366 | 0.3637550363279598 |
| 0.5000000001 | 0.9547562635990923 | 0.16237458193979934 | 0.43728103787875366 | 0.3637552536404438 |
| 0.50000000001 | 0.9547562635990923 | 0.16237458193979934 | 0.43728103787875366 | 0.3637552536404438 |
| 0.500000000001 | 0.9547562635990923 | 0.16237458193979934 | 0.43728103787875366 | 0.3637552536404438 |
| 0.501 | 0.07894661726059203 | 0.16521739130434782 | 0.4369563935417115 | 0.36664537840465017 |
| 0.5003593813663805 | 0.18015837174710114 | 0.1632107023411371 | 0.43696681591301645 | 0.3651031532532713 |
| 0.5001291549665015 | 0.3673963392321741 | 0.16270903010033447 | 0.4370746648615697 | 0.36422704548714674 |
| 0.5000464158883361 | 0.5920213803430702 | 0.16270903010033447 | 0.43733111170037436 | 0.36391700223004286 |
| 0.500016681005372 | 0.7697204337114719 | 0.1623745819397993 | 0.43735522934463766 | 0.3638098161655337 |
| 0.5000059948425032 | 0.8711087893364641 | 0.16237458193979934 | 0.4373071666412955 | 0.36375400554466014 |
| 0.50000215443469 | 0.9201042128690045 | 0.16237458193979934 | 0.43728103787875366 | 0.36375733241888364 |
| 0.5000007742636827 | 0.9414481394214028 | 0.16237458193979934 | 0.43728103787875366 | 0.36375625919355037 |
| 0.5000002782559402 | 0.9499184826743542 | 0.16237458193979934 | 0.43728103787875366 | 0.363756441514514 |
| 0.5000001 | 0.9530700020867431 | 0.16237458193979934 | 0.43728103787875366 | 0.36375643096784677 |


### BM25 + Filter Fuse
- Lost more topics

Avg. Kendall's tau:  0.004242197558995947

| P_10 | bpref | ndcg |
|---|---|---|
| 0.4218068440668467 | 0.10619765494137354 | 0.29136889967380725 | 
