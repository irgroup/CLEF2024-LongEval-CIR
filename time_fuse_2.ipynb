{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jueri/dev/CLEF2024-LongEval-CIR/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import numpy as np\n",
    "import os\n",
    "from repro_eval.Evaluator import RpdEvaluator\n",
    "from repro_eval.util import arp, arp_scores\n",
    "import pytrec_eval\n",
    "import yaml\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Fuse\n",
    "Fuse the new run with one old run by boosting old (known) documents up and new documents down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_fuse(run_recent, run_old, _lambda=0.5):\n",
    "    qid_ranking_groups = run_old.groupby('qid')\n",
    "    qid_ranking_dict = {qid: list(ranking['docno']) for qid, ranking in qid_ranking_groups}\n",
    "    \n",
    "    def weigh(row):\n",
    "        if not qid_ranking_dict.get(row['qid']):\n",
    "            # These topics will be boosted down, an empty set is returned \n",
    "            print(\"Could not find\", row['qid'])\n",
    "            \n",
    "        if row['docno'] in qid_ranking_dict.get(row['qid'], []):\n",
    "            return row['score'] * _lambda ** 2\n",
    "        else:\n",
    "            return row['score'] * (1-_lambda) ** 2           \n",
    "    reranking = run_recent.copy()\n",
    "    \n",
    "    # min max normalization per topic\n",
    "    reranking['score'] = reranking.groupby('qid')['score'].transform(lambda x : x / x.max())\n",
    "    \n",
    "    # weight if in old ranking\n",
    "    reranking['score'] = reranking.progress_apply(weigh, axis=1)\n",
    "    reranking = reranking.sort_values(['qid','score'], ascending=False).groupby('qid').head(1000)\n",
    "    reranking['rank'] = reranking.groupby('qid')['score'].rank(ascending=False).astype(int)\n",
    "    return reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lambda sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"data\"\n",
    "runs_path = \"results/trec\"\n",
    "reranked_path = \"results/fuse_time\"\n",
    "\n",
    "run_new_path = \"CIR_BM25_D-t5_T-t5\"\n",
    "run_old_path = \"CIR_BM25_D-t4_T-t5_extended\"\n",
    "\n",
    "with open(\"data/LongEval/metadata.yml\", \"r\") as yamlfile:\n",
    "    config = yaml.load(yamlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 95953/1463631 [00:04<01:04, 21340.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q08231859\n",
      "Could not find q08231859\n",
      "Could not find q08231859\n",
      "Could not find q08231859\n",
      "Could not find q08231859\n",
      "Could not find q08231859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 214174/1463631 [00:10<01:07, 18423.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q08234008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1463631/1463631 [01:12<00:00, 20317.92it/s]\n"
     ]
    }
   ],
   "source": [
    "run_new = pt.io.read_results(os.path.join(base_path, runs_path, run_new_path))\n",
    "run_old = pt.io.read_results(os.path.join(base_path, runs_path, run_old_path))\n",
    "\n",
    "_lambda = 0.503\n",
    "run_reranked = time_fuse(run_new, run_old, _lambda=_lambda)\n",
    "run_name = run_new_path + f'_ft-{_lambda}'\n",
    "pt.io.write_results(run_reranked, os.path.join(base_path, reranked_path, run_name), format='trec', run_name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 236794/585414 [00:09<00:14, 24754.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585414/585414 [00:23<00:00, 24639.50it/s]\n",
      " 41%|████      | 237381/585414 [00:10<00:12, 28419.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585414/585414 [00:25<00:00, 22622.98it/s]\n",
      " 40%|████      | 236349/585414 [00:08<00:11, 30692.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585414/585414 [00:21<00:00, 26788.12it/s]\n",
      " 40%|████      | 237026/585414 [00:09<00:12, 28277.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585414/585414 [00:22<00:00, 25923.37it/s]\n",
      " 41%|████      | 238275/585414 [00:08<00:11, 30236.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585414/585414 [00:22<00:00, 26228.18it/s]\n",
      " 40%|████      | 236067/585414 [00:08<00:12, 29018.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585414/585414 [00:22<00:00, 26326.50it/s]\n",
      " 41%|████      | 238342/585414 [00:09<00:11, 31247.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585414/585414 [00:22<00:00, 26278.73it/s]\n",
      " 41%|████      | 237105/585414 [00:08<00:11, 30122.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585414/585414 [00:22<00:00, 25508.88it/s]\n",
      " 40%|████      | 237041/585414 [00:09<00:11, 29494.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n",
      "Could not find q012351539607713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 585414/585414 [00:22<00:00, 26183.46it/s]\n"
     ]
    }
   ],
   "source": [
    "run_new = pt.io.read_results(os.path.join(base_path, runs_path, run_new_path))\n",
    "run_old = pt.io.read_results(os.path.join(base_path, runs_path, run_old_path))\n",
    "\n",
    "with open(os.path.join(base_path, config[\"subcollections\"][\"t3\"][\"qrels\"][\"test\"]), \"r\") as f_qrels:\n",
    "    qrels = pytrec_eval.parse_qrel(f_qrels)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, pytrec_eval.supported_measures)\n",
    "\n",
    "results = {}\n",
    "# for l in np.logspace(-4, -6, num=10):\n",
    "# for l in np.logspace(1e-3, 1e-12 num=10):  # sweep from 1e-3 to 1e-12\n",
    "# for l in np.logspace(-10, -1, num=10):\n",
    "# for l in range(1, 11):\n",
    "#     l =  l/100\n",
    "for l in [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009]:\n",
    "    _lambda = 0.5 + l\n",
    "    run_reranked = time_fuse(run_new, run_old, _lambda=_lambda)\n",
    "    results[_lambda] = {}\n",
    "    \n",
    "    # write results\n",
    "    run_name = f'CIR_BM25_D-t3_T-t3_rr-t2-{l}'\n",
    "    run_reranked_path = os.path.join(base_path, reranked_path, run_name)\n",
    "    pt.io.write_results(run_reranked, run_reranked_path, format='trec', run_name=run_name)\n",
    "\n",
    "    # evaluate\n",
    "    rpd_eval = RpdEvaluator(run_b_orig_path=os.path.join(base_path, runs_path, run_new_path), run_b_rep_path=run_reranked_path)\n",
    "    \n",
    "    correlations = rpd_eval.ktau_union().get('baseline')\n",
    "    correlation_scores = [x for x in list(correlations.values()) if ~np.isnan(x)]\n",
    "    avg_tau = sum(correlation_scores) / len(correlation_scores)\n",
    "    results[_lambda][\"tau\"] = avg_tau\n",
    "    \n",
    "    with open(run_reranked_path) as run_reranked:\n",
    "        run = pytrec_eval.parse_run(run_reranked)\n",
    "        scores = evaluator.evaluate(run)\n",
    "        results[_lambda][\"arp\"] = arp_scores(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 0.501 | 0.0789 | 0.1652 | 0.437 | 0.3666 |\n",
      "| 0.502 | 0.0452 | 0.1694 | 0.4361 | 0.3677 |\n",
      "| 0.503 | 0.0348 | 0.1722 | 0.4367 | 0.3686 |\n",
      "| 0.504 | 0.031 | 0.1717 | 0.436 | 0.3691 |\n",
      "| 0.505 | 0.0278 | 0.1712 | 0.4348 | 0.3694 |\n",
      "| 0.506 | 0.0237 | 0.1721 | 0.435 | 0.3694 |\n",
      "| 0.507 | 0.0223 | 0.1722 | 0.4342 | 0.3675 |\n",
      "| 0.508 | 0.0213 | 0.1741 | 0.4346 | 0.3665 |\n",
      "| 0.509 | 0.0205 | 0.1751 | 0.4332 | 0.3657 |\n"
     ]
    }
   ],
   "source": [
    "for l in results.keys():\n",
    "    print(\"|\", \" | \".join([\n",
    "        str(l), \n",
    "        str(round(results[l][\"tau\"], 4)), \n",
    "        str(round(results[l][\"arp\"][\"P_10\"], 4)), \n",
    "        str(round(results[l][\"arp\"][\"bpref\"], 4)), \n",
    "        str(round(results[l][\"arp\"][\"ndcg\"], 4))\n",
    "        ]), \"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from repro_eval.Evaluator import RpdEvaluator\n",
    "from ranx import Run, fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_fuse(run_recent, old_runs: list):\n",
    "    qid_ranking_groups = run_recent.groupby('qid')\n",
    "    qid_ranking_dict_recent = {qid: pd.Series(ranking['score'].values, ranking['docno']).to_dict() for qid, ranking in qid_ranking_groups}\n",
    "    \n",
    "    runs = [Run.from_dict(qid_ranking_dict_recent)]\n",
    "    \n",
    "    for run_old in old_runs:\n",
    "        qid_ranking_groups = run_old.groupby('qid')\n",
    "        qid_ranking_dict_old = {qid: pd.Series(ranking['score'].values, ranking['docno']).to_dict() for qid, ranking in qid_ranking_groups}\n",
    "        for qid, ranking in qid_ranking_dict_old.items():\n",
    "            docs_recent = qid_ranking_dict_recent.get(qid).keys()\n",
    "            qid_ranking_dict_old[qid] = {docid: score for docid, score in ranking.items() if docid in docs_recent}\n",
    "        runs.append(Run.from_dict(qid_ranking_dict_old))\n",
    "    \n",
    "    combined_run = fuse(runs = runs, method = \"rrf\")\n",
    "\n",
    "    return combined_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"data\"\n",
    "runs_path = \"results/trec\"\n",
    "reranked_path = \"results/filter_fuse\"\n",
    "\n",
    "run_new_path = \"CIR_BM25_D-t3_T-t3\"\n",
    "run_old_path = \"CIR_BM25_D-t2_T-t3_extended\"\n",
    "\n",
    "with open(\"data/LongEval/metadata.yml\", \"r\") as yamlfile:\n",
    "    config = yaml.load(yamlfile, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_new = pt.io.read_results(os.path.join(base_path, runs_path, run_new_path))\n",
    "\n",
    "# Load history of runs\n",
    "def load_history_runs(history, sub_collection):\n",
    "    history_index = [\"D-\"+i for i in history]\n",
    "    old_runs = []\n",
    "    for name in os.listdir(os.path.join(base_path, runs_path)):\n",
    "        if sub_collection in name and name.endswith(\"extended\"):\n",
    "            for i in history_index:\n",
    "                if i in name:\n",
    "                    run = pt.io.read_results(os.path.join(base_path, runs_path, name))\n",
    "                    old_runs.append(run)\n",
    "    return old_runs\n",
    "\n",
    "\n",
    "# find core qids\n",
    "def core_topics(run_new, old_runs):\n",
    "    topic_sets = []\n",
    "    for i in old_runs:\n",
    "        topic_sets.append(set(i[\"qid\"]))\n",
    "    topic_sets.append(set(run_new[\"qid\"]))\n",
    "\n",
    "    core_topics = set.intersection(*topic_sets)\n",
    "    print(\"Found known documents for:\", len(core_topics), \"of\", len(run_new[\"qid\"].unique()), \"topics\")\n",
    "    return core_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found known documents for: 597 of 598 topics\n"
     ]
    }
   ],
   "source": [
    "history = [\"t2\"]\n",
    "old_runs = load_history_runs(history, \"t3\")\n",
    "core_topics = core_topics(run_new, old_runs)\n",
    "\n",
    "missing_topics = set(run_new[\"qid\"]) - set(core_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_runs_cleaned = []\n",
    "for run in old_runs:\n",
    "    old_runs_cleaned.append(run[run[\"qid\"].isin(core_topics)])\n",
    "old_runs = old_runs_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_new_cleaned = run_new[run_new[\"qid\"].isin(core_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_reranked = filter_and_fuse(run_new_cleaned, old_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f'CIR_BM25_D-t3_T-t3_rr-ff{\"\".join(history)}'\n",
    "run_reranked_path = os.path.join(base_path, reranked_path, run_name)\n",
    "run_reranked.save(run_reranked_path,  kind='trec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add missing topic back to run\n",
    "missing_topic_ranking = run_new[run_new[\"qid\"].isin(missing_topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_run = pd.read_csv(run_reranked_path, sep=\" \", names=[\"qid\", \"Q0\", \"docno\", \"rank\", \"score\", \"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/f66p4s0s11z2c1c1l3v0qbh40000gn/T/ipykernel_2937/3703733993.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_topic_ranking[\"Q0\"] = \"Q0\"\n"
     ]
    }
   ],
   "source": [
    "missing_topic_ranking[\"Q0\"] = \"Q0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([reranked_run, missing_topic_ranking], ignore_index=True).to_csv(run_reranked_path, sep=\" \", header=None, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 0.00873296943962716 | t2 | 0.1117 | 0.4263 | 0.305 |\n"
     ]
    }
   ],
   "source": [
    "rpd_eval = RpdEvaluator(run_b_orig_path=os.path.join(base_path, runs_path, run_new_path), run_b_rep_path=run_reranked_path)\n",
    "\n",
    "correlations = rpd_eval.ktau_union().get('baseline')\n",
    "correlation_scores = [x for x in list(correlations.values()) if ~np.isnan(x)]\n",
    "\n",
    "avg_tau =  sum(correlation_scores) / len(correlation_scores)\n",
    "\n",
    "with open(run_reranked_path) as run_reranked:\n",
    "    run = pytrec_eval.parse_run(run_reranked)\n",
    "    scores = evaluator.evaluate(run)\n",
    "    print( \"|\", \n",
    "          str(avg_tau), \"|\",\n",
    "          \", \".join(history), \"|\",\n",
    "        str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 0.005752406303693579 | t2, t1 | 0.0987 | 0.4163 | 0.2825 |\n"
     ]
    }
   ],
   "source": [
    "rpd_eval = RpdEvaluator(run_b_orig_path=os.path.join(base_path, runs_path, run_new_path), run_b_rep_path=run_reranked_path)\n",
    "\n",
    "correlations = rpd_eval.ktau_union().get('baseline')\n",
    "correlation_scores = [x for x in list(correlations.values()) if ~np.isnan(x)]\n",
    "\n",
    "avg_tau =  sum(correlation_scores) / len(correlation_scores)\n",
    "\n",
    "with open(run_reranked_path) as run_reranked:\n",
    "    run = pytrec_eval.parse_run(run_reranked)\n",
    "    scores = evaluator.evaluate(run)\n",
    "    print( \"|\", \n",
    "          str(avg_tau), \"|\",\n",
    "          \", \".join(history), \"|\",\n",
    "        str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 0.00580605559250212 | t2, t1, t0 | 0.1007 | 0.4178 | 0.2758 |\n"
     ]
    }
   ],
   "source": [
    "rpd_eval = RpdEvaluator(run_b_orig_path=os.path.join(base_path, runs_path, run_new_path), run_b_rep_path=run_reranked_path)\n",
    "\n",
    "correlations = rpd_eval.ktau_union().get('baseline')\n",
    "correlation_scores = [x for x in list(correlations.values()) if ~np.isnan(x)]\n",
    "\n",
    "avg_tau =  sum(correlation_scores) / len(correlation_scores)\n",
    "\n",
    "with open(run_reranked_path) as run_reranked:\n",
    "    run = pytrec_eval.parse_run(run_reranked)\n",
    "    scores = evaluator.evaluate(run)\n",
    "    print( \"|\", \n",
    "          str(avg_tau), \"|\",\n",
    "          \", \".join(history), \"|\",\n",
    "        str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 0.1776 | 0.4571 | 0.3839 |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(\"/Users/jueri/dev/CLEF2024-LongEval-CIR/data/results/BM25+monoT5/CIR_BM25+monoT5_D-t3_T-t3\") as run_reranked:\n",
    "    run = pytrec_eval.parse_run(run_reranked)\n",
    "    scores = evaluator.evaluate(run)\n",
    "    print( \"|\", \n",
    "        str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
