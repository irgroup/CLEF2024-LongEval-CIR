{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jueri/dev/CLEF2024-LongEval-CIR/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "PyTerrier 0.10.1 has loaded Terrier 5.9 (built by craigm on 2024-05-02 17:40) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import yaml\n",
    "import os\n",
    "from src.load_index import load_index, load_topics, load_qrels, tag\n",
    "from src.extend_runs import extend_run_full\n",
    "import sqlite3\n",
    "from repro_eval.Evaluator import RpdEvaluator\n",
    "import pytrec_eval\n",
    "import numpy as np\n",
    "\n",
    "from repro_eval.util import arp, arp_scores\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loaded index with  2049729 documents.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/LongEval/metadata.yml\", \"r\") as yamlfile:\n",
    "    config = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "results_path = \"data/results/filter_not_rel/\"\n",
    "\n",
    "index = load_index(\"t3\")\n",
    "topics = load_topics(\"t3\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline with 1500 docs per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended baseine for filtering\n",
    "# BM25 = pt.BatchRetrieve(index, wmodel=\"BM25\", verbose=True, num_results=1500)\n",
    "\n",
    "# pt.io.write_results(\n",
    "#     BM25(topics), results_path + f\"/CIR_BM25_D-t3_T-t3-long\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend with docids and qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the run\n",
    "# extend_run_full(results_path + f\"CIR_BM25_D-t3_T-t3-long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend with qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/f66p4s0s11z2c1c1l3v0qbh40000gn/T/ipykernel_17824/3447229843.py:1: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  run = pd.read_csv(results_path + f\"CIR_BM25_D-t3_T-t3-l_extended.t3\", sep=\" \")\n"
     ]
    }
   ],
   "source": [
    "run = pd.read_csv(results_path + f\"CIR_BM25_D-t3_T-t3-l_extended.t3\", sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"data/database.db\")\n",
    "query = \"SELECT * FROM qrel\"\n",
    "qrels = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels[\"key\"] = qrels[\"queryid\"]+qrels[\"docid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmap=qrels[[\"key\", \"relevance\"]].set_index(\"key\").to_dict()[\"relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qrel(row, subcollection):\n",
    "    query_id = row[f\"queryid_{subcollection}\"]\n",
    "    doc_id = row[f\"docid_{subcollection}\"]\n",
    "    if isinstance(query_id, str) and isinstance(doc_id, str):\n",
    "        return qmap.get(row[f\"queryid_{subcollection}\"] + row[f\"docid_{subcollection}\"], None)\n",
    "    else:\n",
    "        return None\n",
    "       \n",
    "for subcollection in [\"t0\", \"t1\", \"t2\", \"t3\"]:\n",
    "    run[f\"qrel_{subcollection}\"] = run.apply(get_qrel, subcollection=subcollection, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds between 450 to 600 qrels per sub-collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter not rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subcollection in [\"t0\", \"t1\", \"t2\"]:\n",
    "history = [\"t2\", \"t1\", \"t0\"]\n",
    "reranked = run.copy()\n",
    "\n",
    "for subcollection in history:\n",
    "    reranked = reranked[reranked[f\"qrel_{subcollection}\"] != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = reranked.sort_values(['queryid','score'], ascending=False).groupby('queryid').head(1000)\n",
    "reranked['rank'] = reranked.groupby('queryid')['score'].rank(ascending=False).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"CIR_BM25_D-t3_T-t3-filter-notrel-{\"\".join(history)}\"\n",
    "run_reranked_path = os.path.join(results_path, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.io.write_results(reranked[[\"queryid\", \"0\", \"docid\", \"score\", \"rank\", \"run\"]].rename(columns={\"queryid\": \"qid\", \"docid\": \"docno\"}), run_reranked_path, format='trec', run_name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"data\"\n",
    "\n",
    "with open(os.path.join(base_path, config[\"subcollections\"][\"t3\"][\"qrels\"][\"test\"]), \"r\") as f_qrels:\n",
    "    qrels = pytrec_eval.parse_qrel(f_qrels)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, pytrec_eval.supported_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| t3 | 0.1798 | 0.7784 | 0.3851 |\n"
     ]
    }
   ],
   "source": [
    "with open(run_reranked_path) as run_reranked:\n",
    "    r = pytrec_eval.parse_run(run_reranked)\n",
    "    scores = evaluator.evaluate(r)\n",
    "    print( \"|\", \n",
    "          \", \".join(history), \"|\",\n",
    "        str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qrel Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qrel_boost(run, history, _lambda=0.5, mu=2):\n",
    "    reranking = run.copy()\n",
    "    \n",
    "    # min max normalization per topic\n",
    "    reranking['score'] = reranking.groupby('queryid')['score'].transform(lambda x : x / x.max())\n",
    "    \n",
    "    for subcollection in history:\n",
    "        # Relevant\n",
    "        reranking.loc[reranking[f\"qrel_{subcollection}\"] == 1, 'score'] = reranking.loc[reranking[f\"qrel_{subcollection}\"] > 0, 'score'] * _lambda ** 2\n",
    "        reranking.loc[reranking[f\"qrel_{subcollection}\"] > 0, 'score'] = reranking.loc[reranking[f\"qrel_{subcollection}\"] > 0, 'score'] * (_lambda ** 2) * mu\n",
    "        \n",
    "        # All Not Relevant\n",
    "        reranking.loc[(reranking[f\"qrel_{subcollection}\"] == 0) | (reranking[f\"qrel_{subcollection}\"].isna()), 'score'] = reranking.loc[(reranking[f\"qrel_{subcollection}\"] == 0) | (reranking[f\"qrel_{subcollection}\"].isna()), 'score'] *(1-_lambda) ** 2\n",
    "\n",
    "        \n",
    "    reranking = reranking.sort_values(['queryid','score'], ascending=False).groupby('queryid').head(1000)\n",
    "    reranking['rank'] = reranking.groupby('queryid')['score'].rank(ascending=False).astype(int)\n",
    "    \n",
    "    return reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| t2 | 0.1 | 0.154 | 0.4231 | 0.349 |\n",
      "| t2 | 0.2 | 0.154 | 0.4231 | 0.349 |\n",
      "| t2 | 0.30000000000000004 | 0.154 | 0.4234 | 0.3492 |\n",
      "| t2 | 0.4 | 0.1543 | 0.4256 | 0.351 |\n",
      "| t2 | 0.5 | 0.1585 | 0.4304 | 0.3622 |\n",
      "| t2 | 0.6 | 0.1776 | 0.4498 | 0.3828 |\n",
      "| t2 | 0.7000000000000001 | 0.1788 | 0.4505 | 0.3837 |\n",
      "| t2 | 0.8 | 0.1788 | 0.4505 | 0.3837 |\n",
      "| t2 | 0.9 | 0.1788 | 0.4505 | 0.3835 |\n"
     ]
    }
   ],
   "source": [
    "base_path = \"data\"\n",
    "with open(os.path.join(base_path, config[\"subcollections\"][\"t3\"][\"qrels\"][\"test\"]), \"r\") as f_qrels:\n",
    "    qrels = pytrec_eval.parse_qrel(f_qrels)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, pytrec_eval.supported_measures)\n",
    "\n",
    "\n",
    "for _lambda in np.arange(0.1, 1, 0.1):\n",
    "    reranked = qrel_boost(run, history, _lambda)\n",
    "    run_name = f\"CIR_BM25_D-t3_T-t3-filter-notrel-{\"\".join(history)}-l{_lambda}\"\n",
    "    run_reranked_path = os.path.join(results_path, run_name)\n",
    "\n",
    "    pt.io.write_results(reranked[[\"queryid\", \"0\", \"docid\", \"score\", \"rank\", \"run\"]].rename(columns={\"queryid\": \"qid\", \"docid\": \"docno\"}), run_reranked_path, format='trec', run_name=run_name)\n",
    "\n",
    "    with open(run_reranked_path) as run_reranked:\n",
    "        r = pytrec_eval.parse_run(run_reranked)\n",
    "        scores = evaluator.evaluate(r)\n",
    "        print( \"|\", \n",
    "              \", \".join(history), \"|\",\n",
    "                      str(_lambda), \"|\",\n",
    "            str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "            str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "            str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| t2, t1 | 0.1 | 0.1513 | 0.4164 | 0.3442 |\n",
      "| t2, t1 | 0.2 | 0.1513 | 0.4164 | 0.3442 |\n",
      "| t2, t1 | 0.30000000000000004 | 0.1513 | 0.4165 | 0.3443 |\n",
      "| t2, t1 | 0.4 | 0.1517 | 0.4185 | 0.3464 |\n",
      "| t2, t1 | 0.5 | 0.1574 | 0.4256 | 0.3598 |\n",
      "| t2, t1 | 0.6 | 0.1828 | 0.4527 | 0.3884 |\n",
      "| t2, t1 | 0.7000000000000001 | 0.1858 | 0.4533 | 0.3909 |\n",
      "| t2, t1 | 0.8 | 0.1858 | 0.4533 | 0.3909 |\n",
      "| t2, t1 | 0.9 | 0.1858 | 0.4533 | 0.3906 |\n"
     ]
    }
   ],
   "source": [
    "base_path = \"data\"\n",
    "with open(os.path.join(base_path, config[\"subcollections\"][\"t3\"][\"qrels\"][\"test\"]), \"r\") as f_qrels:\n",
    "    qrels = pytrec_eval.parse_qrel(f_qrels)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, pytrec_eval.supported_measures)\n",
    "\n",
    "history = [\"t2\", \"t1\"]\n",
    "for _lambda in np.arange(0.1, 1, 0.1):\n",
    "    reranked = qrel_boost(run, history, _lambda)\n",
    "    run_name = f\"CIR_BM25_D-t3_T-t3-filter-notrel-{\"\".join(history)}-l{_lambda}\"\n",
    "    run_reranked_path = os.path.join(results_path, run_name)\n",
    "\n",
    "    pt.io.write_results(reranked[[\"queryid\", \"0\", \"docid\", \"score\", \"rank\", \"run\"]].rename(columns={\"queryid\": \"qid\", \"docid\": \"docno\"}), run_reranked_path, format='trec', run_name=run_name)\n",
    "\n",
    "    with open(run_reranked_path) as run_reranked:\n",
    "        r = pytrec_eval.parse_run(run_reranked)\n",
    "        scores = evaluator.evaluate(r)\n",
    "        print( \"|\", \n",
    "              \", \".join(history), \"|\",\n",
    "                      str(_lambda), \"|\",\n",
    "            str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "            str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "            str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| t2, t1, t0 | 0.1 | 0.1492 | 0.4119 | 0.3407 |\n",
      "| t2, t1, t0 | 0.2 | 0.1492 | 0.4119 | 0.3407 |\n",
      "| t2, t1, t0 | 0.30000000000000004 | 0.1492 | 0.4122 | 0.3409 |\n",
      "| t2, t1, t0 | 0.4 | 0.1495 | 0.4148 | 0.343 |\n",
      "| t2, t1, t0 | 0.5 | 0.1559 | 0.4219 | 0.3571 |\n",
      "| t2, t1, t0 | 0.6 | 0.1858 | 0.4534 | 0.3901 |\n",
      "| t2, t1, t0 | 0.7000000000000001 | 0.1891 | 0.4542 | 0.3928 |\n",
      "| t2, t1, t0 | 0.8 | 0.1891 | 0.4542 | 0.3928 |\n",
      "| t2, t1, t0 | 0.9 | 0.1891 | 0.4541 | 0.3927 |\n"
     ]
    }
   ],
   "source": [
    "base_path = \"data\"\n",
    "with open(os.path.join(base_path, config[\"subcollections\"][\"t3\"][\"qrels\"][\"test\"]), \"r\") as f_qrels:\n",
    "    qrels = pytrec_eval.parse_qrel(f_qrels)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, pytrec_eval.supported_measures)\n",
    "\n",
    "history = [\"t2\", \"t1\", \"t0\"]\n",
    "for _lambda in np.arange(0.1, 1, 0.1):\n",
    "    reranked = qrel_boost(run, history, _lambda)\n",
    "    run_name = f\"CIR_BM25_D-t3_T-t3-filter-notrel-{\"\".join(history)}-l{_lambda}\"\n",
    "    run_reranked_path = os.path.join(results_path, run_name)\n",
    "\n",
    "    pt.io.write_results(reranked[[\"queryid\", \"0\", \"docid\", \"score\", \"rank\", \"run\"]].rename(columns={\"queryid\": \"qid\", \"docid\": \"docno\"}), run_reranked_path, format='trec', run_name=run_name)\n",
    "\n",
    "    with open(run_reranked_path) as run_reranked:\n",
    "        r = pytrec_eval.parse_run(run_reranked)\n",
    "        scores = evaluator.evaluate(r)\n",
    "        print( \"|\", \n",
    "              \", \".join(history), \"|\",\n",
    "                      str(_lambda), \"|\",\n",
    "            str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "            str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "            str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"t2\"]\n",
    "_lambda = 1 + 2\n",
    "reranked = direct_boost(run, history, _lambda=_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"CIR_BM25_D-t3_T-t3-rr-rel-{\"\".join(history)}\"\n",
    "run_reranked_path = os.path.join(results_path, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.io.write_results(reranked[[\"queryid\", \"0\", \"docid\", \"score\", \"rank\", \"run\"]].rename(columns={\"queryid\": \"qid\", \"docid\": \"docno\"}), run_reranked_path, format='trec', run_name=run_name)\n",
    "\n",
    "base_path = \"data\"\n",
    "\n",
    "with open(os.path.join(base_path, config[\"subcollections\"][\"t3\"][\"qrels\"][\"test\"]), \"r\") as f_qrels:\n",
    "    qrels = pytrec_eval.parse_qrel(f_qrels)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, pytrec_eval.supported_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 3 | t2 | 0.1788 | 0.4493 | 0.3822 |\n"
     ]
    }
   ],
   "source": [
    "with open(run_reranked_path) as run_reranked:\n",
    "    r = pytrec_eval.parse_run(run_reranked)\n",
    "    scores = evaluator.evaluate(r)\n",
    "    print( \"|\", \n",
    "          str(_lambda), \"|\",\n",
    "          \", \".join(history), \"|\",\n",
    "        str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
