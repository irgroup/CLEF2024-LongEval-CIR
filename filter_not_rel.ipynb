{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import yaml\n",
    "import os\n",
    "from src.load_index import load_index, load_topics, load_qrels, tag\n",
    "from src.extend_runs import extend_run_full\n",
    "import sqlite3\n",
    "from repro_eval.Evaluator import RpdEvaluator\n",
    "import pytrec_eval\n",
    "import numpy as np\n",
    "\n",
    "from repro_eval.util import arp, arp_scores\n",
    "\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loaded index with  2049729 documents.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/LongEval/metadata.yml\", \"r\") as yamlfile:\n",
    "    config = yaml.load(yamlfile, Loader=yaml.FullLoader)\n",
    "\n",
    "results_path = \"data/results/filter_not_rel/\"\n",
    "\n",
    "index = load_index(\"t3\")\n",
    "topics = load_topics(\"t3\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create baseline with 1500 docs per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended baseine for filtering\n",
    "# BM25 = pt.BatchRetrieve(index, wmodel=\"BM25\", verbose=True, num_results=1500)\n",
    "\n",
    "# pt.io.write_results(\n",
    "#     BM25(topics), results_path + f\"/CIR_BM25_D-t3_T-t3-long\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend with docids and qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the run\n",
    "# extend_run_full(results_path + f\"CIR_BM25_D-t3_T-t3-long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extend with qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/f66p4s0s11z2c1c1l3v0qbh40000gn/T/ipykernel_1335/3447229843.py:1: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  run = pd.read_csv(results_path + f\"CIR_BM25_D-t3_T-t3-l_extended.t3\", sep=\" \")\n"
     ]
    }
   ],
   "source": [
    "run = pd.read_csv(results_path + f\"CIR_BM25_D-t3_T-t3-l_extended.t3\", sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"data/database.db\")\n",
    "query = \"SELECT * FROM qrel\"\n",
    "qrels = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels[\"key\"] = qrels[\"queryid\"]+qrels[\"docid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmap=qrels[[\"key\", \"relevance\"]].set_index(\"key\").to_dict()[\"relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qrel(row, subcollection):\n",
    "    query_id = row[f\"queryid_{subcollection}\"]\n",
    "    doc_id = row[f\"docid_{subcollection}\"]\n",
    "    if isinstance(query_id, str) and isinstance(doc_id, str):\n",
    "        return qmap.get(row[f\"queryid_{subcollection}\"] + row[f\"docid_{subcollection}\"], None)\n",
    "    else:\n",
    "        return None\n",
    "       \n",
    "for subcollection in [\"t0\", \"t1\", \"t2\", \"t3\"]:\n",
    "    run[f\"qrel_{subcollection}\"] = run.apply(get_qrel, subcollection=subcollection, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds between 450 to 600 qrels per sub-collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter not rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for subcollection in [\"t0\", \"t1\", \"t2\"]:\n",
    "history = [\"t2\", \"t1\", \"t0\"]\n",
    "reranked = run.copy()\n",
    "\n",
    "for subcollection in history:\n",
    "    reranked = reranked[reranked[f\"qrel_{subcollection}\"] != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = reranked.sort_values(['queryid','score'], ascending=False).groupby('queryid').head(1000)\n",
    "reranked['rank'] = reranked.groupby('queryid')['score'].rank(ascending=False).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"CIR_BM25_D-t3_T-t3-filter-notrel-{\"\".join(history)}\"\n",
    "run_reranked_path = os.path.join(results_path, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.io.write_results(reranked[[\"queryid\", \"0\", \"docid\", \"score\", \"rank\", \"run\"]].rename(columns={\"queryid\": \"qid\", \"docid\": \"docno\"}), run_reranked_path, format='trec', run_name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"data\"\n",
    "\n",
    "with open(os.path.join(base_path, config[\"subcollections\"][\"t3\"][\"qrels\"][\"test\"]), \"r\") as f_qrels:\n",
    "    qrels = pytrec_eval.parse_qrel(f_qrels)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, pytrec_eval.supported_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| t3 | 0.1798 | 0.7784 | 0.3851 |\n"
     ]
    }
   ],
   "source": [
    "with open(run_reranked_path) as run_reranked:\n",
    "    r = pytrec_eval.parse_run(run_reranked)\n",
    "    scores = evaluator.evaluate(r)\n",
    "    print( \"|\", \n",
    "          \", \".join(history), \"|\",\n",
    "        str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boostrel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_boost(run, history, _lambda=1):\n",
    "    reranking = run.copy()\n",
    "    \n",
    "    # min max normalization per topic\n",
    "    reranking['score'] = reranking.groupby('queryid')['score'].transform(lambda x : x / x.max())\n",
    "    \n",
    "    for subcollection in history:\n",
    "        # All Relevant\n",
    "        reranking.loc[reranking[f\"qrel_{subcollection}\"] > 0, 'score'] = reranking.loc[reranking[f\"qrel_{subcollection}\"] > 0, 'score'] * _lambda ** 2\n",
    "        \n",
    "    reranking = reranking.sort_values(['queryid','score'], ascending=False).groupby('queryid').head(1000)\n",
    "    reranking['rank'] = reranking.groupby('queryid')['score'].rank(ascending=False).astype(int)\n",
    "    \n",
    "    return reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\"t2\"]\n",
    "_lambda = 1 + 2\n",
    "reranked = direct_boost(run, history, _lambda=_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"CIR_BM25_D-t3_T-t3-rr-rel-{\"\".join(history)}\"\n",
    "run_reranked_path = os.path.join(results_path, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt.io.write_results(reranked[[\"queryid\", \"0\", \"docid\", \"score\", \"rank\", \"run\"]].rename(columns={\"queryid\": \"qid\", \"docid\": \"docno\"}), run_reranked_path, format='trec', run_name=run_name)\n",
    "\n",
    "base_path = \"data\"\n",
    "\n",
    "with open(os.path.join(base_path, config[\"subcollections\"][\"t3\"][\"qrels\"][\"test\"]), \"r\") as f_qrels:\n",
    "    qrels = pytrec_eval.parse_qrel(f_qrels)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, pytrec_eval.supported_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 3 | t2 | 0.1788 | 0.4493 | 0.3822 |\n"
     ]
    }
   ],
   "source": [
    "with open(run_reranked_path) as run_reranked:\n",
    "    r = pytrec_eval.parse_run(run_reranked)\n",
    "    scores = evaluator.evaluate(r)\n",
    "    print( \"|\", \n",
    "          str(_lambda), \"|\",\n",
    "          \", \".join(history), \"|\",\n",
    "        str(round(arp_scores(scores)[\"P_10\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"bpref\"], 4)), \"|\",\n",
    "        str(round(arp_scores(scores)[\"ndcg\"], 4)), \"|\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
